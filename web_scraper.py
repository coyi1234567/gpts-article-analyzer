"""
ğŸŒ ç½‘é¡µå†…å®¹æŠ“å–å™¨
æ”¯æŒå¤šå¹³å°æ–‡ç« å†…å®¹æå–ï¼ŒåŒ…æ‹¬æ–‡å­—å’Œå›¾ç‰‡

æ”¯æŒå¹³å°ï¼š
- å¾®ä¿¡å…¬ä¼—å¹³å°
- å°çº¢ä¹¦
- çŸ¥ä¹
- å¾®åš
- ä»Šæ—¥å¤´æ¡
- å…¶ä»–ä¸»æµå¹³å°
"""

import re
import logging
from typing import Dict, List, Optional
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup

from config import Config

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class WebScraper:
    """ç½‘é¡µå†…å®¹æŠ“å–å™¨ï¼Œæ”¯æŒå›¾æ–‡å†…å®¹æå–"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': Config.USER_AGENT,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.timeout = Config.TIMEOUT
        
        # å¹³å°ç‰¹å®šçš„é€‰æ‹©å™¨é…ç½®
        self.platform_selectors = {
            'wechat': {
                'title': ['h1', '.rich_media_title', '#activity-name'],
                'content': ['#js_content', '.rich_media_content'],
                'author': ['.rich_media_meta_text', '.profile_nickname'],
                'time': ['.rich_media_meta_text', '#publish_time']
            },
            'zhihu': {
                'title': ['h1', '.QuestionHeader-title'],
                'content': ['.RichContent', '.AnswerItem'],
                'author': ['.AuthorInfo-name', '.UserLink-link'],
                'time': ['.ContentItem-time']
            },
            'weibo': {
                'title': ['h1', '.WB_text'],
                'content': ['.WB_text', '.WB_detail'],
                'author': ['.WB_info', '.WB_name'],
                'time': ['.WB_from', '.WB_time']
            },
            'xiaohongshu': {
                'title': ['.title', '.note-title'],
                'content': ['.content', '.note-content'],
                'author': ['.author', '.user-name'],
                'time': ['.time', '.publish-time']
            }
        }
    
    def scrape_article(self, url: str) -> Dict:
        """
        æŠ“å–æ–‡ç« å†…å®¹ï¼ŒåŒ…æ‹¬æ–‡å­—å’Œå›¾ç‰‡
        
        Args:
            url: æ–‡ç« é“¾æ¥
            
        Returns:
            åŒ…å«æ–‡ç« ä¿¡æ¯çš„å­—å…¸
        """
        try:
            logger.info(f"å¼€å§‹æŠ“å–æ–‡ç« : {url}")
            
            # å‘é€è¯·æ±‚
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            
            # è§£æHTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # è¯†åˆ«å¹³å°
            platform = self._identify_platform(url)
            
            # æå–æ–‡ç« ä¿¡æ¯
            article_info = {
                'url': url,
                'platform': platform,
                'title': self._extract_title(soup, platform),
                'content': self._extract_content(soup, platform),
                'images': self._extract_images(soup, url),
                'author': self._extract_author(soup, platform),
                'publish_time': self._extract_publish_time(soup, platform),
                'summary': self._extract_summary(soup),
                'tags': self._extract_tags(soup),
                'word_count': 0,  # å°†åœ¨å†…å®¹æå–åè®¡ç®—
                'image_count': 0  # å°†åœ¨å›¾ç‰‡æå–åè®¡ç®—
            }
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            article_info['word_count'] = len(article_info['content'])
            article_info['image_count'] = len(article_info['images'])
            
            logger.info(f"æ–‡ç« æŠ“å–å®Œæˆ: {article_info['title']} (å­—æ•°: {article_info['word_count']}, å›¾ç‰‡: {article_info['image_count']})")
            return article_info
            
        except requests.exceptions.RequestException as e:
            logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {url}, é”™è¯¯: {str(e)}")
            return self._create_error_response(url, f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
        except Exception as e:
            logger.error(f"æŠ“å–æ–‡ç« å¤±è´¥: {url}, é”™è¯¯: {str(e)}")
            return self._create_error_response(url, str(e))
    
    def _identify_platform(self, url: str) -> str:
        """è¯†åˆ«æ–‡ç« å¹³å°"""
        domain = urlparse(url).netloc.lower()
        
        if 'mp.weixin.qq.com' in domain:
            return 'wechat'
        elif 'zhihu.com' in domain:
            return 'zhihu'
        elif 'weibo.com' in domain:
            return 'weibo'
        elif 'xiaohongshu.com' in domain:
            return 'xiaohongshu'
        elif 'toutiao.com' in domain:
            return 'toutiao'
        else:
            return 'other'
    
    def _extract_title(self, soup: BeautifulSoup, platform: str) -> str:
        """æå–æ–‡ç« æ ‡é¢˜"""
        # å°è¯•å¹³å°ç‰¹å®šé€‰æ‹©å™¨
        if platform in self.platform_selectors:
            for selector in self.platform_selectors[platform]['title']:
                title_elem = soup.select_one(selector)
                if title_elem and title_elem.get_text(strip=True):
                    return title_elem.get_text(strip=True)
        
        # é€šç”¨é€‰æ‹©å™¨
        title_selectors = [
            'h1',
            '.article-title',
            '.post-title',
            '.entry-title',
            'title',
            '[class*="title"]',
            '[id*="title"]'
        ]
        
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem and title_elem.get_text(strip=True):
                title = title_elem.get_text(strip=True)
                # è¿‡æ»¤æ‰ç½‘ç«™åç§°
                if ' - ' in title:
                    title = title.split(' - ')[0]
                return title
        
        return soup.title.get_text(strip=True) if soup.title else ""
    
    def _extract_content(self, soup: BeautifulSoup, platform: str) -> str:
        """æå–æ–‡ç« æ­£æ–‡å†…å®¹"""
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement', 'ad']):
            element.decompose()
        
        # å°è¯•å¹³å°ç‰¹å®šé€‰æ‹©å™¨
        if platform in self.platform_selectors:
            for selector in self.platform_selectors[platform]['content']:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content = self._clean_text(content_elem.get_text())
                    if len(content) > 100:  # ç¡®ä¿å†…å®¹è¶³å¤Ÿé•¿
                        return content
        
        # é€šç”¨å†…å®¹é€‰æ‹©å™¨
        content_selectors = [
            '.article-content',
            '.post-content',
            '.entry-content',
            '.content',
            'article',
            '.article-body',
            '.post-body',
            '[class*="content"]',
            '[id*="content"]'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = self._clean_text(content_elem.get_text())
                if len(content) > 100:  # ç¡®ä¿å†…å®¹è¶³å¤Ÿé•¿
                    return content
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šå®¹å™¨ï¼Œå°è¯•ä»bodyä¸­æå–
        body = soup.find('body')
        if body:
            return self._clean_text(body.get_text())
        
        return ""
    
    def _extract_images(self, soup: BeautifulSoup, base_url: str) -> List[Dict]:
        """æå–æ–‡ç« ä¸­çš„å›¾ç‰‡"""
        images = []
        
        # 1. æŸ¥æ‰¾æ‰€æœ‰imgæ ‡ç­¾
        img_tags = soup.find_all('img')
        
        # 2. æŸ¥æ‰¾å¾®ä¿¡æ–‡ç« ç‰¹æœ‰çš„å›¾ç‰‡å…ƒç´ 
        wechat_imgs = soup.find_all(['img', 'div'], attrs={'data-src': True})
        
        # 3. æŸ¥æ‰¾èƒŒæ™¯å›¾ç‰‡
        bg_imgs = soup.find_all(attrs={'style': re.compile(r'background-image')})
        
        # åˆå¹¶æ‰€æœ‰å›¾ç‰‡å…ƒç´ 
        all_img_elements = list(img_tags) + list(wechat_imgs) + list(bg_imgs)
        
        for img in all_img_elements:
            img_info = {
                'src': '',
                'alt': '',
                'title': '',
                'width': '',
                'height': '',
                'absolute_url': '',
                'type': 'img_tag'
            }
            
            # è·å–å›¾ç‰‡å±æ€§
            if img.name == 'img':
                img_info['src'] = img.get('src', '') or img.get('data-src', '') or img.get('data-original', '')
                img_info['alt'] = img.get('alt', '')
                img_info['title'] = img.get('title', '')
                img_info['width'] = img.get('width', '')
                img_info['height'] = img.get('height', '')
                img_info['type'] = 'img_tag'
            else:
                # å¤„ç†å…¶ä»–å…ƒç´ 
                img_info['src'] = img.get('data-src', '') or img.get('data-original', '')
                img_info['alt'] = img.get('alt', '')
                img_info['title'] = img.get('title', '')
                img_info['type'] = 'data_src'
                
                # å¤„ç†èƒŒæ™¯å›¾ç‰‡
                style = img.get('style', '')
                if 'background-image' in style:
                    bg_match = re.search(r'background-image:\s*url\(["\']?([^"\']+)["\']?\)', style)
                    if bg_match:
                        img_info['src'] = bg_match.group(1)
                        img_info['type'] = 'background_image'
            
            # è½¬æ¢ä¸ºç»å¯¹URL
            if img_info['src']:
                img_info['absolute_url'] = urljoin(base_url, img_info['src'])
                
                # è¿‡æ»¤æ‰å°å›¾æ ‡å’Œè£…é¥°æ€§å›¾ç‰‡
                if self._is_valid_content_image(img_info):
                    images.append(img_info)
        
        # å»é‡ï¼ˆåŸºäºURLï¼‰
        seen_urls = set()
        unique_images = []
        for img in images:
            if img['absolute_url'] not in seen_urls:
                seen_urls.add(img['absolute_url'])
                unique_images.append(img)
        
        logger.info(f"æ‰¾åˆ° {len(unique_images)} å¼ æœ‰æ•ˆå›¾ç‰‡")
        return unique_images
    
    def _is_valid_content_image(self, img_info: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å†…å®¹å›¾ç‰‡"""
        src = img_info['src'].lower()
        alt = img_info['alt'].lower()
        title = img_info.get('title', '').lower()
        
        # è¿‡æ»¤æ‰æ˜æ˜¾çš„éå†…å®¹å›¾ç‰‡
        exclude_patterns = [
            'logo', 'icon', 'button', 'banner', 'ad', 'advertisement',
            'sponsor', 'sponsored', 'sidebar', 'header', 'footer', 'nav',
            'social', 'share', 'comment', 'like', 'follow', 'loading',
            'placeholder', 'blank', 'transparent', 'qrcode', 'qr-code',
            'wechat', 'weixin', 'follow', 'subscribe', 'decorative', 'divider'
        ]
        
        # æ£€æŸ¥URLã€altã€titleä¸­æ˜¯å¦åŒ…å«æ’é™¤æ¨¡å¼
        for pattern in exclude_patterns:
            if pattern in src or pattern in alt or pattern in title:
                return False
        
        # è¿‡æ»¤æ‰å¤ªå°çš„å›¾ç‰‡ï¼ˆå¯èƒ½æ˜¯è£…é¥°æ€§å›¾ç‰‡ï¼‰
        try:
            width = int(img_info['width']) if img_info['width'] else 0
            height = int(img_info['height']) if img_info['height'] else 0
            if width > 0 and height > 0 and (width < 100 or height < 100):
                return False
        except:
            pass
        
        # ä¿ç•™å°é¢å›¾ç‰‡
        if 'cover' in alt or 'cover' in title:
            return True
        
        # å¯¹äºå¾®ä¿¡æ–‡ç« ï¼Œè¿›ä¸€æ­¥åˆ¤æ–­
        if 'mp.weixin.qq.com' in src or 'mmecoa.qpic.cn' in src or 'mmbiz.qpic.cn' in src:
            # å¾®ä¿¡æ–‡ç« çš„å›¾ç‰‡ï¼Œä½†æ’é™¤æ˜æ˜¾çš„å¤´åƒå’Œè£…é¥°å›¾ç‰‡
            if not alt and not title:
                try:
                    width = int(img_info['width']) if img_info['width'] else 0
                    height = int(img_info['height']) if img_info['height'] else 0
                    if width > 0 and height > 0 and (width < 200 or height < 200):
                        return False
                except:
                    pass
            
            # å¯¹äºå¾®ä¿¡æ–‡ç« ï¼Œå¦‚æœå›¾ç‰‡URLåŒ…å«ç‰¹å®šæ¨¡å¼ï¼Œå¯èƒ½æ˜¯è£…é¥°å›¾ç‰‡
            decorative_patterns = [
                'avatar', 'head', 'profile', 'icon', 'logo', 'banner', 'ad'
            ]
            for pattern in decorative_patterns:
                if pattern in src:
                    return False
            
            return True
        
        return True
    
    def _extract_author(self, soup: BeautifulSoup, platform: str) -> str:
        """æå–ä½œè€…ä¿¡æ¯"""
        # å°è¯•å¹³å°ç‰¹å®šé€‰æ‹©å™¨
        if platform in self.platform_selectors:
            for selector in self.platform_selectors[platform]['author']:
                author_elem = soup.select_one(selector)
                if author_elem:
                    return author_elem.get_text(strip=True)
        
        # é€šç”¨é€‰æ‹©å™¨
        author_selectors = [
            '.author',
            '.byline',
            '.writer',
            '[class*="author"]',
            '[class*="byline"]',
            'meta[name="author"]'
        ]
        
        for selector in author_selectors:
            if selector.startswith('meta'):
                author_elem = soup.select_one(selector)
                if author_elem:
                    return author_elem.get('content', '')
            else:
                author_elem = soup.select_one(selector)
                if author_elem:
                    return author_elem.get_text(strip=True)
        
        return ""
    
    def _extract_publish_time(self, soup: BeautifulSoup, platform: str) -> str:
        """æå–å‘å¸ƒæ—¶é—´"""
        # å°è¯•å¹³å°ç‰¹å®šé€‰æ‹©å™¨
        if platform in self.platform_selectors:
            for selector in self.platform_selectors[platform]['time']:
                time_elem = soup.select_one(selector)
                if time_elem:
                    return time_elem.get_text(strip=True)
        
        # é€šç”¨é€‰æ‹©å™¨
        time_selectors = [
            '.publish-time',
            '.post-time',
            '.date',
            'time',
            '[class*="time"]',
            '[class*="date"]',
            'meta[property="article:published_time"]',
            'meta[name="publishdate"]'
        ]
        
        for selector in time_selectors:
            if selector.startswith('meta'):
                time_elem = soup.select_one(selector)
                if time_elem:
                    return time_elem.get('content', '')
            else:
                time_elem = soup.select_one(selector)
                if time_elem:
                    return time_elem.get_text(strip=True)
        
        return ""
    
    def _extract_summary(self, soup: BeautifulSoup) -> str:
        """æå–æ–‡ç« æ‘˜è¦"""
        summary_selectors = [
            '.summary',
            '.excerpt',
            '.description',
            'meta[name="description"]',
            'meta[property="og:description"]'
        ]
        
        for selector in summary_selectors:
            if selector.startswith('meta'):
                summary_elem = soup.select_one(selector)
                if summary_elem:
                    return summary_elem.get('content', '')
            else:
                summary_elem = soup.select_one(selector)
                if summary_elem:
                    return summary_elem.get_text(strip=True)
        
        return ""
    
    def _extract_tags(self, soup: BeautifulSoup) -> List[str]:
        """æå–æ–‡ç« æ ‡ç­¾"""
        tags = []
        
        # å°è¯•å¤šç§æ ‡ç­¾é€‰æ‹©å™¨
        tag_selectors = [
            '.tags a',
            '.tag',
            '.category',
            '.keywords',
            'meta[name="keywords"]'
        ]
        
        for selector in tag_selectors:
            if selector.startswith('meta'):
                tag_elem = soup.select_one(selector)
                if tag_elem:
                    keywords = tag_elem.get('content', '')
                    if keywords:
                        tags.extend([tag.strip() for tag in keywords.split(',')])
            else:
                tag_elems = soup.select(selector)
                for tag_elem in tag_elems:
                    tag_text = tag_elem.get_text(strip=True)
                    if tag_text:
                        tags.append(tag_text)
        
        return list(set(tags))  # å»é‡
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:()ï¼ˆï¼‰ã€ã€‘""''""''ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]', '', text)
        return text.strip()
    
    def _create_error_response(self, url: str, error_message: str) -> Dict:
        """åˆ›å»ºé”™è¯¯å“åº”"""
        return {
            'url': url,
            'error': error_message,
            'title': '',
            'content': '',
            'images': [],
            'author': '',
            'publish_time': '',
            'summary': '',
            'tags': [],
            'platform': 'unknown',
            'word_count': 0,
            'image_count': 0
        }


# æµ‹è¯•å‡½æ•°
if __name__ == "__main__":
    scraper = WebScraper()
    
    # æµ‹è¯•å¾®ä¿¡æ–‡ç« 
    test_url = "https://mp.weixin.qq.com/s/example"
    result = scraper.scrape_article(test_url)
    
    print("æ–‡ç« æ ‡é¢˜:", result['title'])
    print("æ–‡ç« å†…å®¹é•¿åº¦:", len(result['content']))
    print("å›¾ç‰‡æ•°é‡:", len(result['images']))
    print("ä½œè€…:", result['author'])
    print("å‘å¸ƒæ—¶é—´:", result['publish_time'])
    print("å¹³å°:", result['platform'])